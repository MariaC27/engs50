https://thayer.github.io/engs50/Labs/Lab4/DESIGN.html
3
11351
<!DOCTYPEhtml><html><head><metacharset="utf-8"><metahttp-equiv="X-UA-Compatible"content="IE=edge"><metaname="viewport"content="width=device-width,initial-scale=1"><title>TSECrawlerDesignSpec</title><metaname="description"content="InEngs50youwilllearnhowtodesign&buildlarge,reliable,maintainable,andunderstandablesoftwaresystems.IntheprocessyouwilllearntoprograminCwithUnixdevelopmenttools."><linkrel="shortcuticon"href="/engs50/50.png"/><linkrel="stylesheet"href="/engs50/css/main.css"><linkrel="canonical"href="/engs50/Labs/Lab4/DESIGN.html"></head><body><headerclass="site-header"><aclass="site-title"href="/engs50/"><imgwidth=48align=centersrc="/engs50/50.png"alt="icon">Engs50(CS50)</a><navclass="site-nav">[<ahref="/engs50/Labs/">Labs</a>][<ahref="/engs50/Notes/">Notes</a>][<ahref="/engs50/Reading/">Reading</a>][<ahref="/engs50/Resources/">Resources</a>][<ahref="/engs50/Examples/">Examples</a>]</nav></header><divclass="page-content"><divclass="wrapper"><articleclass="post"><headerclass="post-header"><h1class="post-title">TSECrawlerDesignSpec</h1></header><divclass="post-content"><p>Recallthe<ahref="REQUIREMENTS.html">CrawlerRequirementsSpec</a>;the<strong>crawler</strong>crawlsawebsiteandretrieveswebpagesstartingwithaspecifiedURL.Itparsestheinitialwebpage,extractsanyembeddedURLsandretrievesthosepages,andcrawlsthepagesfoundatthoseURLs,butlimitsitselfto<codeclass="language-plaintexthighlighter-rouge">maxDepth</code>hopsfromtheseedURLandtoURLsthatare‘internal’tothedesignatedCS50server.Whenthecrawlerprocessiscomplete,theindexingofthecollecteddocumentscanbegin.</p><p>ADesignSpecshouldcontainseveralsections:</p><ul><li>Userinterface</li><li>InputsandOutputs</li><li>Functionaldecompositionintomodules</li><li>Pseudocodeforlogic/algorithmicflow</li><li>Dataflowthroughmodules</li><li>Majordatastructures</li><li>Testingplan</li></ul><p>Let’slookthrougheach.</p><h3id="user-interface">Userinterface</h3><p>Thecrawler’sonlyinterfacewiththeuserisonthecommand-line;itmustalwayshavethreearguments.</p><divclass="language-plaintexthighlighter-rouge"><divclass="highlight"><preclass="highlight"><code>crawlerseedURLpageDirectorymaxDepth</code></pre></div></div><p>Forexample:</p><divclass="language-bashhighlighter-rouge"><divclass="highlight"><preclass="highlight"><code><spanclass="nv">$</span>crawlerhttp://old-www.cs.dartmouth.edu/index.html./data/2</code></pre></div></div><h3id="inputs-and-outputs">Inputsandoutputs</h3><p>Input:theonlyinputsarecommand-lineparameters;seetheUserInterfaceabove.</p><p>Output:Wesaveeachexploredwebpagetoafile,onefileperpage.WeuseauniquedocumentIDasthefilename,fordocumentIDs1,2,3,4,andsoforth.Withinafile,wewrite</p><ul><li>thepageURLonthefirstline,</li><li>thedepthofthepage(wheretheseedisdepth0)onthesecondline,</li><li>thepagecontents,beginningonthethirdline.</li></ul><h3id="functional-decomposition-into-modules">Functionaldecompositionintomodules</h3><p>Weanticipatethefollowingmodulesorfunctions:</p><ol><li><em>main</em>,whichparsesargumentsandinitializesothermodules</li><li><em>crawler</em>,whichloopsoverpagestoexplore,untilthelistisexhausted</li><li><em>pagefetcher</em>,whichfetchesapagefromaURL</li><li><em>pagescanner</em>,whichextractsURLsfromapageandprocesseseachone</li><li><em>pagesaver</em>,whichoutputsapagetothetheappropriatefile</li></ol><p>Andsomehelpermodulesthatprovidedatastructures:</p><ol><li><em>bag</em>ofpageswehaveyettoexplore</li><li><em>hashtable</em>ofURLswe’veseensofar</li></ol><h3id="pseudo-code-for-logicalgorithmic-flow">Pseudocodeforlogic/algorithmicflow</h3><p>Thecrawlerwillrunasfollows:</p><ol><li>executefromacommandlineasshownintheUserInterface</li><li>parsethecommandline,validateparameters,initializeothermodules</li><li>makea<em>webpage</em>forthe<codeclass="language-plaintexthighlighter-rouge">seedURL</code>,markedwithdepth=0</li><li>addthatpagetothe<em>bag</em>ofwebpagestocrawl</li><li>addthatURLtothe<em>hashtable</em>ofURLsseen</li><li>whiletherearemorewebpagestocrawl,<ol><li>extractawebpage(URL,depth)itemfromthe<em>bag</em>ofwebpagestobecrawled,</li><li>pauseforatleastonesecond,</li><li>use<em>pagefetcher</em>toretrieveawebpageforthatURL,</li><li>use<em>pagesaver</em>towritethewebpagetothe<codeclass="language-plaintexthighlighter-rouge">pageDirectory</code>withauniquedocumentID,asdescribedintheRequirements.</li><li>ifthewebpagedepthis&lt;<codeclass="language-plaintexthighlighter-rouge">maxDepth</code>,explorethewebpagetofindlinks:<ol><li>use<em>pagescanner</em>toparsethewebpagetoextractallitsembeddedURLs;</li><li>foreachextractedURL,<ol><li>‘normalize’theURL(seebelow)</li><li>ifthatURLisnot‘internal’(seebelow),ignoreit;</li><li>trytoinsertthatURLintothe<em>hashtable</em>ofURLsseen<ol><li>ifitwasalreadyinthetable,donothing;</li><li>ifitwasaddedtothetable,<ol><li>makeanew<em>webpage</em>forthatURL,atdepth+1</li><li>addthenewwebpagetothe<em>bag</em>ofwebpagestobecrawled</li></ol></li></ol></li></ol></li></ol></li></ol></li></ol><p><strong>normalizetheURL</strong>meanstoconvertitintoaclean,canonicalform.</p><p><strong>internal</strong>meanstheURLstayswithintheCS50playgroundhttp://old-www.cs.dartmouth.edu/.</p><p>Agoodimplementationwillnotnecessarilyencodealltheabovecodeinasingle,deeply-nestedfunction;partoftheImplementationSpecistobreakthepseudocodedownintoacleanlyarrangedsetoffunctions.</p><p>Noticethatourpseudocodesaysnothingabouttheorderinwhichitcrawlswebpages;sinceitpresumablypullsthemoutofa<em>bag</em>,anda<em>bag</em>abstractdatastructureexplicitlydeniesanypromiseabouttheorderofitemsremovedfromabag,wecan’texpectanyparticularcrawlorder.That’sok.TheresultmayormaynotbeaBreadth-FirstSearch,butforthecrawlerwedon’tcareabouttheorderaslongasweexploreeverythingwithinthe<codeclass="language-plaintexthighlighter-rouge">maxDepth</code>neighborhood.</p><p>Thecrawlercompletesandexitswhenithasnothingleftinitsbag-nomorepagestobecrawled.ThemaxDepthparameterindirectlydeterminesthenumberofpagesthatthecrawlerwillretrieve.</p><h3id="dataflow-through-modules">Dataflowthroughmodules</h3><ol><li><em>main</em>parsesparametersandpassesthemtothecrawler.</li><li><em>crawler</em>usesabagtotrackpagestoexplore,andhashtabletotrackpagesseen;whenitexploresapageitgivesthepageURLtothepagefetcher,thentheresulttopagesaver,thentothepagescanner.</li><li><em>pagefetcher</em>fetchesthecontents(HTML)forapagefromaURLandreturns.</li><li><em>pagesaver</em>outputsapagetotheappropriatefile.</li><li><em>pagescanner</em>extractsURLsfromapageandreturnsoneatatime.</li></ol><h3id="major-data-structures">Majordatastructures</h3><p>Threehelpermodulesprovidedatastructures:</p><ol><li><em>bag</em>ofpage(URL,depth)structures</li><li><em>set</em>ofURLs(indirectlyusedbyhashtable)</li><li><em>hashtable</em>ofURLs</li></ol><h3id="testing-plan">Testingplan</h3><p><em>Unittesting</em>.Asmalltestprogramtotesteachmoduletomakesureitdoeswhatit’ssupposedtodo.</p><p><em>Integrationtesting</em>.Assemblethecrawlerandtestitasawhole.Ineachcase,examinetheoutputfilescarefullytobesuretheyhavethecontentsofthecorrectpage,withthe correctURL,andthecorrectdepth.Ensurethatnopagesaremissingorduplicated.Print“progress”indicatorsfromthecrawlerasitproceeds(e.g.,printeachURLexplored,andeachURLfoundinthepagesitexplores)soyoucanwatchitsprogressasitruns.</p><ol><li><p>Testtheprogramwithvariousformsofincorrectcommand-lineargumentstoensurethatitscommand-lineparsing,andvalidationofthoseparameters,workscorrectly.</p></li><li><p>Testthecrawlerwitha<codeclass="language-plaintexthighlighter-rouge">seedURL</code>thatpointstoanon-existentserver.</p></li><li><p>Testthecrawlerwitha<codeclass="language-plaintexthighlighter-rouge">seedURL</code>thatpointstoanon-internalserver.</p></li><li><p>Testthecrawlerwitha<codeclass="language-plaintexthighlighter-rouge">seedURL</code>thatpointstoavalidserverbutnon-existentpage.</p></li><li><p>Crawlasimple,closedsetofcross-linkedwebpagestocrawl.Ensurethatsomepage(s)arementionedmultipletimeswithinapage,andmultipletimesacrossthesetofpages.Ensurethereisaloop(acycleinthegraphofpages).Insuchalittlesite,youknowexactlywhatsetofpagesshouldbecrawled,atwhatdepths,andyouknowwhereyourprogrammighttripup.</p></li><li><p>Pointthecrawleratapageinthatsite,andexploreatdepths0,1,2,3,4,5.Verifythatthefilescreatedmatchexpectations.</p></li><li><p>Repeatwithadifferentseedpageinthatsamesite.Ifthesiteisindeedagraph,withcycles,thereshouldbeseveralinterestingstartingpoints.</p></li><li><p>PointthecrawleratourWikipediaplayground.Exploreatdepths0,1,2.(Ittakesalongtimetorunatdepth2orhigher!)Verifythatthefilescreatedmatchexpectations.</p></li><li><p>Whenyouareconfidentthatyourcrawlerrunswell,testitonapartofourplaygroundorwithagreaterdepth-butbereadytokillitifitseemstoberunningamok.</p></li></ol></div></article></div></div><footerclass="site-footer"><divclass="wrapper"><h2class="footer-heading">Engs50(CS50)--DartmouthCollege</h2><p><fontsize=-1>ThisversionofthecourseisbaseduponthosedesignedbyProfessorsPalmer,Kotz,Zhou,Campbell,andBalkcom.Iamdeeplyindebtedtotheseoutstandingeducators.--<ahref="https://engineering.dartmouth.edu/people/faculty/stephen-taylor/">StephenTaylor</a></font></p><p><small>Thispagewaslastupdatedon<strong>2023-01-05</strong>at<strong>11:31</strong>.</small></p></div></footer></body></html>
